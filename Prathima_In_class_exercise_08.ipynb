{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Prathima_In_class_exercise_08.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pn0159/prathima_INFO5731_Fall2020/blob/master/Prathima_In_class_exercise_08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh5eVw5c-bbS"
      },
      "source": [
        "# **The eighth in-class-exercise (20 points in total, 10/29/2020)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaZZ8pDX-bbS"
      },
      "source": [
        "The data for this exercise is from the dataset you created from assignment three. Please perform answer the following questions based on your data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgfk1Tg_-bbT"
      },
      "source": [
        "## (1) (10 points) Write a python program to extract the sentiment related terms from the corpus. You may use python package such as polyglot or external lexicon resources in the question. Rank the sentiment related terms by frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4s-8SM7-bbU",
        "outputId": "6bc1ed01-39cb-4a5e-a2ed-6d7be7aec8a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pytreebank\n",
        "import pytreebank\n",
        "import sys\n",
        "import os\n",
        "out_path = os.path.join(sys.path[0],'sst_{}.txt')\n",
        "dataset = pytreebank.load_sst('./cleaned_data')\n",
        "for category in ['train', 'test']:\n",
        "    with open(out_path.format(category), 'w') as outfile:\n",
        "        for item in dataset[category]:\n",
        "            outfile.write(\"__label__{}\\t{}\\n\".format(\n",
        "                item.to_labeled_lines()[0][0] + 1,\n",
        "                item.to_labeled_lines()[0][1]\n",
        "            ))\n",
        "\n",
        "print(len(dataset['train']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytreebank in /usr/local/lib/python3.6/dist-packages (0.2.7)\n",
            "8544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1cLRPMyQ8QR",
        "outputId": "978837bf-48ee-45da-c85e-e16e564678c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "from textblob import TextBlob\n",
        "\n",
        "my1data = pd.read_csv(\"/content/sample_data/mydata.csv\")\n",
        "my1data = my1data[['Abstract']]\n",
        "my1data['Abstract'] = my1data['Abstract'].str.replace('[^\\w\\s]','')\n",
        "my1data['Abstract'] = my1data['Abstract'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "my1data['Abstract'] = my1data['Abstract'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "my1data['Abstract'] = my1data['Abstract'].apply(lambda x: nltk.word_tokenize(x))\n",
        "mydata_freq = (my1data['Abstract']).apply(lambda x: pd.value_counts(x)).sum(axis = 0).reset_index()\n",
        "mydata_freq.columns = ['List of Words', 'Value of tf']\n",
        "mydata_freq['Value of polarity'] = mydata_freq['List of Words'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "senti_terms = mydata_freq.loc[mydata_freq['Value of polarity'] != 0].sort_values(by='Value of tf', ascending=False)\n",
        "senti_terms = senti_terms.reset_index(drop=True)\n",
        "print('Ranking of sentiment related terms by frequency:')\n",
        "senti_terms"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Ranking of sentiment related terms by frequency:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>List of Words</th>\n",
              "      <th>Value of tf</th>\n",
              "      <th>Value of polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>natural</td>\n",
              "      <td>71.0</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>linguistic</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>new</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.136364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>many</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>effective</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>cultural</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>conventional</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.142857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>contemporary</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>fit</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>kind</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>90 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   List of Words  Value of tf  Value of polarity\n",
              "0        natural         71.0           0.100000\n",
              "1     linguistic         10.0           0.100000\n",
              "2            new          8.0           0.136364\n",
              "3           many          7.0           0.500000\n",
              "4      effective          7.0           0.600000\n",
              "..           ...          ...                ...\n",
              "85      cultural          1.0           0.100000\n",
              "86  conventional          1.0          -0.142857\n",
              "87  contemporary          1.0           0.166667\n",
              "88           fit          1.0           0.400000\n",
              "89          kind          1.0           0.600000\n",
              "\n",
              "[90 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc3jOKoQ-bbX"
      },
      "source": [
        "## (2) (10 points) Compare the performance of the following tools in sentiment identification: TextBlob (https://textblob.readthedocs.io/en/dev/), VADER (https://github.com/cjhutto/vaderSentiment), TFIDF-based Support Vector Machine (SVM) (Split your data into training and testing data). Take your own annotation as the standard answers. \n",
        "\n",
        "Reference code: https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQrvWbX1HR-1"
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"/content/sample_data/cleaned_data.csv\",usecols=[\"cleaned_text\",\"Sentiment\"])\n",
        "df=df.dropna().reset_index()"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNISWm-2yxTO",
        "outputId": "577cdfad-c97c-4e24-c508-4b7a2f1307bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>introduction statistical natural language proc...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>the paper summarizes essential property docume...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>abstract language way communicating word langu...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>we report experiment use standard natural lang...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>paper describe simple rule based approach auto...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>95</td>\n",
              "      <td>bibliography</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>96</td>\n",
              "      <td>applying natural language processing technique...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>97</td>\n",
              "      <td>example natural language chinese english itali...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>98</td>\n",
              "      <td>a number powerful modelling technique develope...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>99</td>\n",
              "      <td>a semi automated approach design database enha...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>93 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    index                                       cleaned_text Sentiment\n",
              "0       0  introduction statistical natural language proc...  positive\n",
              "1       1  the paper summarizes essential property docume...  positive\n",
              "2       2  abstract language way communicating word langu...  positive\n",
              "3       3  we report experiment use standard natural lang...  positive\n",
              "4       4  paper describe simple rule based approach auto...  positive\n",
              "..    ...                                                ...       ...\n",
              "88     95                                       bibliography  positive\n",
              "89     96  applying natural language processing technique...  positive\n",
              "90     97  example natural language chinese english itali...  positive\n",
              "91     98  a number powerful modelling technique develope...  positive\n",
              "92     99  a semi automated approach design database enha...  positive\n",
              "\n",
              "[93 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4eQLsLpc4l0",
        "outputId": "58e06fc4-3a4c-407b-a156-a56212123750",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "textblob_data = pd.read_csv(\"/content/sample_data/myfinaldata.csv\")\n",
        "textblob_data['polarity'] = textblob_data['Abstract'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "textblob_data['predicted sentiment'] = pd.cut(textblob_data['polarity'], bins=4, labels=[1, 2, 3, 4])\n",
        "#Defining sentiment\n",
        "def sentiment(x):\n",
        "    if x in [1, 2]:\n",
        "        return 'Negative'\n",
        "    if x == 3:\n",
        "        return 'Neutral'\n",
        "    if x == 4:\n",
        "        return 'Positive'\n",
        "textblob_data['predicted sentiment'] = textblob_data['predicted sentiment'].apply(lambda x: sentiment(x))\n",
        "print(textblob_data[['documentid', 'sentiment', 'predicted sentiment']].head(5))\n",
        "textblob_accuracy = accuracy_score(textblob_data['sentiment'], textblob_data['predicted sentiment'])*100\n",
        "textblob_f1 = f1_score(textblob_data['sentiment'], textblob_data['predicted sentiment'], average='macro')\n",
        "print(' f1-score of the TextBlob:', textblob_f1)\n"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   documentid sentiment predicted sentiment\n",
            "0           0  positive             Neutral\n",
            "1           1  positive             Neutral\n",
            "2           2  positive             Neutral\n",
            "3           3  positive             Neutral\n",
            "4           4  positive             Neutral\n",
            " f1-score of the TextBlob: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flA3hjJvg18r",
        "outputId": "fa7b61ad-23eb-456e-f30d-6d564a8d4555",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "vader = SentimentIntensityAnalyzer()\n",
        "\n",
        "myvader_data = pd.read_csv(\"/content/sample_data/myfinaldata.csv\")\n",
        "myvader_data['polarity'] = textblob_data['Abstract'].apply(lambda x: vader.polarity_scores(x)['compound'])\n",
        "myvader_data['predicted sentiment'] = pd.cut(myvader_data['polarity'], bins=4, labels=[1, 2, 3, 4])\n",
        "myvader_data['predicted sentiment'] = myvader_data['predicted sentiment'].apply(lambda x: sentiment(x))\n",
        "print(myvader_data[['documentid', 'sentiment', 'predicted sentiment']].head(5))\n",
        "vader_accuracy = accuracy_score(myvader_data['sentiment'], myvader_data['predicted sentiment'])*100\n",
        "vader_f1 = f1_score(myvader_data['sentiment'], myvader_data['predicted sentiment'], average='macro')\n",
        "print(' f1-score of the VADER sentiment identification :', vader_f1)\n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "   documentid sentiment predicted sentiment\n",
            "0           0  positive             Neutral\n",
            "1           1  positive            Positive\n",
            "2           2  positive            Positive\n",
            "3           3  positive            Positive\n",
            "4           4  positive            Negative\n",
            " f1-score of the VADER sentiment identification : 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNc5NfjAhRMA",
        "outputId": "dd21aa28-c044-4314-9925-114b133b550e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "mysvm = pd.read_csv(\"/content/sample_data/myfinaldata.csv\")\n",
        "train, test = sklearn.model_selection.train_test_split(mysvm, train_size=0.6, test_size=0.1)\n",
        "mypip = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=100, \n",
        "                                           learning_rate='optimal', tol=None))])\n",
        "\n",
        "svm = mypip.fit(train['Abstract'], train['sentiment'])\n",
        "test['predicted sentiment'] = svm.predict(test['Abstract'])\n",
        "print(test[['documentid', 'sentiment', 'predicted sentiment']].head(5))\n",
        "\n",
        "mysvm_acc = accuracy_score(test['sentiment'], test['predicted sentiment'])*100\n",
        "mysvm_f1 = f1_score(test['sentiment'], test['predicted sentiment'], average='macro')\n",
        "\n",
        "print('Accuracy of the TFIDF-based SVM:', mysvm_acc)\n",
        "print('f1-score of the TFIDF-based SVM :', mysvm_f1)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    documentid sentiment predicted sentiment\n",
            "70          70  positive            positive\n",
            "96          96  positive            positive\n",
            "58          58  positive            positive\n",
            "15          15  positive            positive\n",
            "0            0  positive            positive\n",
            "Accuracy of the TFIDF-based SVM: 90.0\n",
            "f1-score of the TFIDF-based SVM : 0.4736842105263158\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqX-D5Ot3vrc"
      },
      "source": [
        "\"\"\"\n",
        "Sentiment Analysis can be done by using \n",
        "1.TextBlob\n",
        "2.VADER \n",
        "3.TF-IDF based SVM. \n",
        "\n",
        "TextBlob:TextBlob is a popular Python library for processing textual data. It is built on top of NLTK, another popular Natural Language Processing toolbox for Python. \n",
        "TextBlob uses a sentiment lexicon (consisting of predefined words) to assign scores for each word, which are then averaged out to give an overall sentence sentiment score.\n",
        "\n",
        "VADER: Valence Aware Dictionary and sEntiment Reasoner is another popular rule-based library for sentiment analysis.\n",
        "Like TextBlob, it uses a sentiment lexicon that contains intensity measures for each word based on human-annotated labels. A key difference however, is that VADER was designed with a focus on social m\n",
        "This means that it puts a lot of emphasis on rules that capture the essence of text typically seen on social media\n",
        "\n",
        "SVM:TF-IDF is a numerical statistic that is intended to reflect how important a word is to a text document.\n",
        "Support Vector Machine (SVM) is one of the most frequently used classification algorithms in text categorization.\n",
        "It can deal with linear and non-linear data classification with the help of kernel function\n",
        "\n",
        "The performance of the models can be measured using accuracy score and f1 score. \n",
        "\n",
        "Analysis:\n",
        "From my analysis,TF-IDF Based SVM has given the best accuracy and f1 score, with 90% and 0.47 respectively,\n",
        "which means out of the three models i have used for doing sentiment analysis.SVM is the best model for sentiment analysis. \n",
        "VADER and Textblob was not at all goodto identify. \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}